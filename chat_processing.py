import logging
import ollama 

def process_general_chat(user_input):
    """
    Processes a general conversation input using an LLM.
    """
    try:
        response = ollama.chat(model='qwen2.5:3B', messages=[{"role": "user", "content": user_input}])
        chat_response = response['message']['content']
        logging.info("General chat response generated by LLM.")
        return chat_response
    except Exception as e:
        logging.error(f"Error in general chat processing: {e}")
        return "Sorry, something went wrong processing your message."
